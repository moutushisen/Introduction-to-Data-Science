{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Problem 1: Loss Function for Poisson Regression\n",
    "def poisson_loss(X, y, alpha, beta):\n",
    "    lambda_ = np.exp(np.dot(X, alpha) + beta)\n",
    "    loss = np.sum(lambda_ - y * (np.dot(X, alpha) + beta))\n",
    "    return loss\n",
    "\n",
    "# Example Usage\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])  # Example data\n",
    "y = np.array([2, 3, 1])\n",
    "alpha = np.array([0.1, 0.2])\n",
    "beta = 0.3\n",
    "loss = poisson_loss(X, y, alpha, beta)\n",
    "print(f\"Poisson Loss: {loss}\")\n",
    "\n",
    "# Problem 2: Distribution of theta_hat\n",
    "def uniform_theta_hat_distribution(n, theta):\n",
    "    x = np.linspace(0, theta, 1000)\n",
    "    cdf = (x / theta) ** n\n",
    "    return x, cdf\n",
    "\n",
    "def compute_bias_variance_mse(n, theta):\n",
    "    expected_theta_hat = (n / (n + 1)) * theta\n",
    "    bias = expected_theta_hat - theta\n",
    "    variance = (theta**2 * n) / ((n + 1)**2 * (n + 2))\n",
    "    mse = bias**2 + variance\n",
    "    return bias, variance, mse\n",
    "\n",
    "# Example Usage\n",
    "n = 5\n",
    "theta = 10\n",
    "x, cdf = uniform_theta_hat_distribution(n, theta)\n",
    "bias, variance, mse = compute_bias_variance_mse(n, theta)\n",
    "print(f\"Bias: {bias}, Variance: {variance}, MSE: {mse}\")\n",
    "\n",
    "# Problem 3: Continuous Distribution\n",
    "def distribution_function(x):\n",
    "    return 0.5 * (np.sin(x) + 1)\n",
    "\n",
    "def inverse_distribution_function(u):\n",
    "    return np.arcsin(2 * u - 1)\n",
    "\n",
    "def accept_reject_sampler(size):\n",
    "    samples = []\n",
    "    M = np.pi / 2\n",
    "    while len(samples) < size:\n",
    "        x = np.random.uniform(-np.pi / 2, np.pi / 2)\n",
    "        u = np.random.uniform(0, M * 1 / np.pi)\n",
    "        if u <= 0.5 * np.cos(x):\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "\n",
    "# Example Usage\n",
    "samples = accept_reject_sampler(1000)\n",
    "plt.hist(samples, bins=30, density=True, alpha=0.7, label='Sampled')\n",
    "x = np.linspace(-np.pi / 2, np.pi / 2, 1000)\n",
    "plt.plot(x, 0.5 * np.cos(x), label='True Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Problem 4: Markov Chain Transition Matrix\n",
    "def compute_transition_matrix():\n",
    "    probabilities = [0.1, 0.3, 0.2, 0.4]\n",
    "    n_states = len(probabilities)\n",
    "    P = np.zeros((n_states, n_states))\n",
    "    for i in range(n_states):\n",
    "        for j in range(i, n_states):\n",
    "            P[i, j] = probabilities[j] / sum(probabilities[i:])\n",
    "    return P\n",
    "\n",
    "# Example Usage\n",
    "P = compute_transition_matrix()\n",
    "print(\"Transition Matrix:\")\n",
    "print(P)\n",
    "\n",
    "# Problem 5: Empirical Quantiles and Confidence Interval\n",
    "def empirical_quantile(data, p):\n",
    "    data_sorted = np.sort(data)\n",
    "    index = int(np.ceil(p * len(data))) - 1\n",
    "    return data_sorted[index]\n",
    "\n",
    "def confidence_interval_quantile(data, p, alpha=0.05):\n",
    "    n = len(data)\n",
    "    z = stats.norm.ppf(1 - alpha / 2)\n",
    "    q_hat = empirical_quantile(data, p)\n",
    "    se = np.sqrt(p * (1 - p) / n)\n",
    "    ci_lower = q_hat - z * se\n",
    "    ci_upper = q_hat + z * se\n",
    "    return q_hat, (ci_lower, ci_upper)\n",
    "\n",
    "# Example Usage\n",
    "data = np.random.uniform(0, 1, 1000)\n",
    "p = 0.5\n",
    "q_hat, ci = confidence_interval_quantile(data, p)\n",
    "print(f\"Estimated Quantile: {q_hat}, Confidence Interval: {ci}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
