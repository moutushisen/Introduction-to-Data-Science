{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "\n",
    "# Part 1: 3 points\n",
    "\n",
    "# Load the data from the file data/websites.csv and estimate the transition matrix of the Markov chain\n",
    "# Store the estimated transition matrix in the variable problem2_transition_matrix below\n",
    "# problem2_transition_matrix = XXX # A numpy array of shape (problem2_n_states, problem2_n_states)\n",
    "# Store the number of states in the variable problem2_n_states below\n",
    "# problem2_n_states = XXX # An integer\n",
    "# ==================================== #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/websites.csv')\n",
    "\n",
    "# Find the number of states (pages)\n",
    "pages = np.union1d(df['source'].unique(), df['destination'].unique())\n",
    "problem2_n_states = int(pages.max() + 1)  \n",
    "\n",
    "# Initialize transition count matrix\n",
    "transition_counts = np.zeros((problem2_n_states, problem2_n_states), dtype=int)\n",
    "\n",
    "# Fill transition counts\n",
    "for _, row in df.iterrows():\n",
    "    transition_counts[int(row['source']), int(row['destination'])] += 1\n",
    "\n",
    "# Normalize to get the transition probability matrix\n",
    "row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "# Avoid division by zero\n",
    "row_sums[row_sums == 0] = 1\n",
    "problem2_transition_matrix = transition_counts / row_sums\n",
    "\n",
    "# Part 2: 4 points\n",
    "\n",
    "# Simulate the website load times for the next page of 10000 users that are currently on page 8 (recall indexing starts at 0) when we only preload the most likely page.\n",
    "# Store the simulated page load times in the variable problem2_page_load_times_top below\n",
    "# problem2_page_load_times_top = XXX # A numpy array of shape (10000,)\n",
    "\n",
    "# Repeat the simulation of load times for the next page of 10000 users that are currently on page 8 when we preload the two most likely pages.\n",
    "# Store the simulated page load times in the variable problem2_page_load_times_two below\n",
    "# problem2_page_load_times_two = XXX # A numpy array of shape (10000,)\n",
    "\n",
    "# ===================================== #\n",
    "# For reproducibility\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_sim = 10000\n",
    "start_page = 8\n",
    "\n",
    "# Probabilities of transitions from page 8\n",
    "probs_from_8 = problem2_transition_matrix[start_page]\n",
    "\n",
    "# Draw the next page for each user\n",
    "next_pages = np.random.choice(problem2_n_states, size=n_sim, p=probs_from_8)\n",
    "\n",
    "# Preload only the most likely next site\n",
    "most_likely = np.argmax(probs_from_8)\n",
    "problem2_page_load_times_top = np.where(\n",
    "    next_pages == most_likely,\n",
    "    np.random.exponential(1/20, n_sim),\n",
    "    np.random.exponential(1/3, n_sim)\n",
    ")\n",
    "\n",
    "# Preload the two most likely pages\n",
    "top_two = np.argsort(probs_from_8)[-2:]\n",
    "problem2_page_load_times_two = np.where(\n",
    "    np.isin(next_pages, top_two),\n",
    "    np.random.exponential(1/20, n_sim),\n",
    "    np.random.exponential(1/3, n_sim)\n",
    ")\n",
    "\n",
    "# Part 3: 3 points\n",
    "\n",
    "# Calculate the true expected load time for loading a page without pre-loading the next page and store it in the variable below\n",
    "#problem2_avg = XXX # A float\n",
    "\n",
    "# Is the average load time for loading a page without pre-loading the next page larger than the average load time for loading a page after pre-loading the next most likely page?\n",
    "# problem2_comparison = XXX # True / False\n",
    "\n",
    "# =================================  #\n",
    "\n",
    "# Theoretical expected load time without pre-loading\n",
    "problem2_avg = 1/3  # mean of Exp(3)\n",
    "\n",
    "# Is the average load time with no pre-load larger than with preloading the top page?\n",
    "problem2_comparison = problem2_avg > problem2_page_load_times_top.mean()\n",
    "\n",
    "\n",
    "# Part 4: 4 points\n",
    "\n",
    "# Begin by calculating the stationary distribution of the Markov chain and store it in the variable below\n",
    "# WARNING: Since the transition matrix is not symmetric, numpy might make the output of the eigenvectors complex, you can use np.real() to get the real part of the eigenvectors\n",
    "# Store the stationary distribution in the variable below called problem2_stationary_distribution\n",
    "# problem2_stationary_distribution = XXX # A numpy array of shape (problem2_n_states,)\n",
    "\n",
    "# Now use the above stationary distribution to calculate the average load time for loading a page after pre-loading the next most likely page according to the stationary distribution\n",
    "# Store the average load time in the variable below\n",
    "# problem2_avg_stationary = XXX # A float\n",
    "\n",
    "# ===============================  #\n",
    "\n",
    "# Calculate stationary distribution (left eigenvector of eigenvalue 1)\n",
    "eigvals, eigvecs = np.linalg.eig(problem2_transition_matrix.T)\n",
    "stat_idx = np.isclose(eigvals, 1)\n",
    "stationary = np.real(eigvecs[:, stat_idx][:, 0])\n",
    "problem2_stationary_distribution = stationary / stationary.sum()\n",
    "\n",
    "# For each state, find the most probable next page\n",
    "most_likely_next = np.argmax(problem2_transition_matrix, axis=1)\n",
    "\n",
    "# Expected load time when preloading the most likely page from each state\n",
    "expected_load_time_by_state = (\n",
    "    problem2_transition_matrix[np.arange(problem2_n_states), most_likely_next] * (1/20) +\n",
    "    (1 - problem2_transition_matrix[np.arange(problem2_n_states), most_likely_next]) * (1/3)\n",
    ")\n",
    "problem2_avg_stationary = np.dot(problem2_stationary_distribution, expected_load_time_by_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free text answer\n",
    "\n",
    "Put the explanation for **part 3** of how you made the decision about `problem2_comparison` below this line in this **cell**. In order to enter edit mode you can doubleclick this cell or select it and press enter.\n",
    "The decision for `problem2_comparison` is based on comparing the theoretical average load time for loading a page without pre-loading (which is the mean of an Exp(3) distribution, i.e., 1/3 seconds) to the empirical average load time obtained from the simulation where we preload the most likely next page (`problem2_page_load_times_top`). If the empirical average load time with preloading is **lower** than the theoretical average load time without preloading, then `problem2_comparison` is set to `True`, indicating that preloading the most likely page improves the average load time. Otherwise, it is `False`. This comparison is made directly by evaluating `problem2_avg > problem2_page_load_times_top.mean()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "\n",
    "\n",
    "# Part 1\n",
    "\n",
    "# def problem1_rejection(n_samples=1):\n",
    "    # Distribution from part 1\n",
    "    # write the code in this function to produce samples from the distribution in the assignment\n",
    "    # Make sure you choose a good sampling distribution to avoid unnecessary rejections\n",
    "\n",
    "    # Return a numpy array of length n_samples\n",
    "    # return XXX\n",
    "\n",
    "# ==========================  #\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples=1):\n",
    "    samples = []\n",
    "    M = 2 * (np.exp(1) - 1) / (np.exp(1) - 2)\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = 2 * x * (np.exp(x**2) - 1) / (np.exp(1) - 2)\n",
    "        if u < fx / M:\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "# Part 2\n",
    "\n",
    "# problem1_samples = XXX\n",
    "# ==========================  #\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "problem1_samples = problem1_rejection(100000)\n",
    "\n",
    "# Plot histogram and true density\n",
    "x_grid = np.linspace(0, 1, 200)\n",
    "true_density = 2 * x_grid * (np.exp(x_grid**2) - 1) / (np.exp(1) - 2)\n",
    "\n",
    "plt.hist(problem1_samples, bins=50, density=True, alpha=0.5, label='Samples')\n",
    "plt.plot(x_grid, true_density, 'r-', label='True density')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.title('Histogram of samples vs true density')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part 3\n",
    "\n",
    "# problem1_integral = XXX\n",
    "# ==========================  #\n",
    "problem1_integral = np.mean(np.sin(problem1_samples))\n",
    "\n",
    "\n",
    "# Part 4\n",
    "\n",
    "# problem1_interval = [XXX,XXX]\n",
    "# ==========================  #\n",
    "\n",
    "n = len(problem1_samples)\n",
    "a = 0\n",
    "b = np.sin(1)\n",
    "mean = problem1_integral\n",
    "delta = np.sqrt(np.log(2/0.05) * (b - a) ** 2 / (2 * n))\n",
    "problem1_interval = [mean - delta, mean + delta]\n",
    "\n",
    "\n",
    "\n",
    "# Part 5\n",
    "\n",
    "# def problem1_rejection_2(n_samples=1):\n",
    "    # Distribution from part 2\n",
    "    # write the code in this function to produce samples from the distribution in the assignment\n",
    "    # Make sure you choose a good sampling distribution to avoid unnecessary rejections\n",
    "\n",
    "    # Return a numpy array of length n_samples\n",
    "    # return XXX\n",
    "\n",
    "# ==========================  #\n",
    "\n",
    "def problem1_rejection_2(n_samples=1):\n",
    "    samples = []\n",
    "    M = 21\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1/20)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = 20 * np.exp(20 - 1/x) * (1 + 1/x)\n",
    "        gx = 20  # Uniform on [0, 1/20]\n",
    "        if u < fx / (M * gx):\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "# Summary all variables\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples=1):\n",
    "    samples = []\n",
    "    M = 2 * (np.exp(1) - 1) / (np.exp(1) - 2)\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = 2 * x * (np.exp(x**2) - 1) / (np.exp(1) - 2)\n",
    "        if u < fx / M:\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "\n",
    "problem1_samples = problem1_rejection(100000)\n",
    "problem1_integral = np.mean(np.sin(problem1_samples))\n",
    "\n",
    "n = len(problem1_samples)\n",
    "a = 0\n",
    "b = np.sin(1)\n",
    "mean = problem1_integral\n",
    "delta = np.sqrt(np.log(2/0.05) * (b - a) ** 2 / (2 * n))\n",
    "problem1_interval = [mean - delta, mean + delta]\n",
    "\n",
    "def problem1_rejection_2(n_samples=1):\n",
    "    samples = []\n",
    "    M = 21\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1/20)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = 20 * np.exp(20 - 1/x) * (1 + 1/x)\n",
    "        gx = 20\n",
    "        if u < fx / (M * gx):\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "\n",
    "# Part 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost(y_true, y_predict_proba, threshold):\n",
    "    # Predict class labels based on threshold\n",
    "    y_pred = (y_predict_proba >= threshold).astype(int)\n",
    "    # Calculate confusion matrix components\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    # Compute total cost\n",
    "    total_cost = (TP * 100) + (FP * 120) + (FN * 600)  # TN has zero cost\n",
    "    avg_cost = total_cost / len(y_true)\n",
    "    return avg_cost\n",
    "\n",
    "# Plot cost vs threshold\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "costs = [cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, t) for t in thresholds]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, costs, label=\"Cost\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Average Cost\")\n",
    "plt.title(\"Average Cost vs Threshold\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part 2\n",
    "\n",
    "# Find threshold that minimizes cost\n",
    "min_idx = np.argmin(costs)\n",
    "problem3_threshold = thresholds[min_idx]\n",
    "\n",
    "# Get cost at optimal threshold\n",
    "problem3_cost_val = costs[min_idx]\n",
    "\n",
    "# Predicted labels at optimal threshold\n",
    "problem3_y_pred_val = (PROBLEM3_y_pred_proba_val >= problem3_threshold).astype(int)\n",
    "\n",
    "# Precision/Recall for class 1\n",
    "TP = np.sum((problem3_y_pred_val == 1) & (PROBLEM3_y_true_val == 1))\n",
    "FP = np.sum((problem3_y_pred_val == 1) & (PROBLEM3_y_true_val == 0))\n",
    "FN = np.sum((problem3_y_pred_val == 0) & (PROBLEM3_y_true_val == 1))\n",
    "TN = np.sum((problem3_y_pred_val == 0) & (PROBLEM3_y_true_val == 0))\n",
    "\n",
    "problem3_precision_1 = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "problem3_recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "# Precision/Recall for class 0\n",
    "problem3_precision_0 = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "problem3_recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "\n",
    "# Part 3\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 0-1 loss: minimize error rate\n",
    "accuracies = [accuracy_score(PROBLEM3_y_true_val, (PROBLEM3_y_pred_proba_val >= t).astype(int)) for t in thresholds]\n",
    "problem3_threshold_01 = thresholds[np.argmax(accuracies)]\n",
    "\n",
    "# Cost difference\n",
    "cost_at_01 = cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, problem3_threshold_01)\n",
    "problem3_cost_difference = abs(cost_at_01 - problem3_cost_val)\n",
    "\n",
    "# Part 4\n",
    "\n",
    "# Use threshold that minimizes cost (problem3_threshold)\n",
    "test_costs = []\n",
    "y_pred_test = (PROBLEM3_y_pred_proba_test >= problem3_threshold).astype(int)\n",
    "# Per-sample cost\n",
    "for yt, yp in zip(PROBLEM3_y_true_test, y_pred_test):\n",
    "    if yp == 1 and yt == 1:\n",
    "        test_costs.append(100)\n",
    "    elif yp == 0 and yt == 0:\n",
    "        test_costs.append(0)\n",
    "    elif yp == 1 and yt == 0:\n",
    "        test_costs.append(120)\n",
    "    elif yp == 0 and yt == 1:\n",
    "        test_costs.append(600)\n",
    "test_costs = np.array(test_costs)\n",
    "n = len(test_costs)\n",
    "empirical_mean = np.mean(test_costs)\n",
    "\n",
    "# Hoeffding's inequality: with probability >= 1-delta, mean is in [mean - eps, mean + eps]\n",
    "# For 95% CI, delta = 0.05\n",
    "from math import sqrt, log\n",
    "a, b = 0, 600  # min and max possible per-sample cost\n",
    "delta = 0.05\n",
    "eps = (b - a) * sqrt(log(2/delta) / (2*n))\n",
    "problem3_lower_bound = empirical_mean - eps\n",
    "problem3_upper_bound = empirical_mean + eps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put your explanation for part 4 below this line in this **cell**. Doubleclick to enter edit mode as before.\n",
    "\n",
    "To construct the 95% confidence interval for the average cost on the test data, we used Hoeffding’s inequality, which is valid for independent, bounded random variables. Since each transaction’s cost is between 0 and 600, and assuming the test samples are independent, Hoeffding’s inequality gives a range around the observed mean cost where the true mean cost will lie with 95% confidence. This provides a distribution-free, reliable interval for the model’s average cost performance on new, similar data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Problem 2\n",
    "# ============================================== #\n",
    "# CORRECTIONS : Page 1 should replaced by Page 8 #\n",
    "# ============================================== #\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are just numbers in the dataset $0,1,2,\\ldots$ and each row consists of a user, a source and a destination page. This signifies that the user was on the source page and clicked a link leading them to the destination page. The goal is to improve the user experience by decreasing load time of the next page visited, as such we need a good estimate for the next site likely to be visited. We will model this using a homogeneous Markov chain, each row in the data-file then corresponds to a single realization of a transition. \n",
    "\n",
    "1. [3p] Load the data in the file `data/websites.csv` and construct a matrix of size `n_pages x n_pages` which is the maximum likelihood estimate of the true transition matrix for the Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is page $0$ has index $0$ in the matrix.\n",
    "2. [4p] A page loads in $\\text{Exp}(3)$ (Exponentially distributed with mean $1/3$) seconds if not preloaded and loads with $\\text{Exp}(20)$ (Exponentially distributed with mean $1/20$) seconds if preloaded and we only preload the most likely next site. Given that we start in page $8$ simulate $10000$ load times from page $8$ (that is, only a single step), store the result in the variable indicated in the cell.\n",
    "Repeat the experiment but this time preload the two most likely pages and store the result in the indicated variable.\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in the free text field).\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load time with respect to the stationary distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "\n",
    "1. [4p] Fill in the remaining part of the function `problem1_rejection` in order to produce samples from the below distribution using rejection sampling: (Hint: $F$ is the distribution function)\n",
    "\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        \\frac{e^{x^2}-x^2-1}{e-2}, & 0 < x < 1 \\\\\n",
    "        1, & x \\geq 1\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "2. [2p] Produce 100000 samples (**use fewer if it takes too long (more than 10 sec) and you cannot find a solution**) and put the answer in `problem1_samples` from the above distribution and plot the histogram together with the true density. \n",
    "3. [2p] Use the above 100000 samples (`problem1_samples`) to approximately compute the integral\n",
    "\n",
    "$$\n",
    "    \\int_0^{1} \\sin(x) \\frac{2(e^{x^2}-1) x}{e-2} dx\n",
    "$$\n",
    "and store the result in `problem1_integral`.\n",
    "\n",
    "4. [2p] Use Hoeffdings inequality to produce a 95\\% confidence interval of the integral above and store the result as a tuple in the variable `problem1_interval`\n",
    "\n",
    "5. [4p] Fill in the remaining part of the function `problem1_rejection_2` in order to produce samples from the below distribution using rejection sampling:\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        20xe^{20-1/x}, & 0 < x < \\frac{1}{20} \\\\\n",
    "        1, & x \\geq \\frac{1}{20}\n",
    "    \\end{cases}\n",
    "$$\n",
    "Hint: this is tricky because if you choose the wrong sampling distribution you reject at least 9 times out of 10. You will get points based on how long your code takes to create a certain number of samples, if you choose the correct sampling distribution you can easily create 100000 samples within 2 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "* **True Positive (TP)**: Detecting fraud and blocking the transaction costs the company 100 (manual review etc.)\n",
    "* **True Negative (TN)**: Allowing a legitimate transaction has no cost.\n",
    "* **False Positive (FP)**: Incorrectly classifying a legitimate transaction as fraudulent costs 120 (customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "* **False Negative (FN)**: Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "**The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES**\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on the validation data. Also calculate the precision and recall at the optimal threshold on the validation data on class 1 and 0.\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the $0-1$ loss. Calculate the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "3. [4p] Provide a confidence interval around the optimal cost (with $95\\%$ confidence) applied to the test data and explain all the assumption you made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Your code for all the sub-parts of the rejection sampling problem looks **correct and well-structured**. Here’s a review and feedback for each part:\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 1: `problem1_rejection`**\n",
    "\n",
    "Your implementation:\n",
    "```python\n",
    "def problem1_rejection(n_samples=1):\n",
    "    samples = []\n",
    "    M = 2 * (np.exp(1) - 1) / (np.exp(1) - 2)\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = 2 * x * (np.exp(x**2) - 1) / (np.exp(1) - 2)\n",
    "        if u < fx / M:\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "```\n",
    "- **Correct logic**: You use uniform sampling on [0, 1], compute the target density, and accept/reject based on the ratio.  \n",
    "- **Good choice of M**: You found the correct upper bound for `fx` over the support.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 2: Sampling and Plot**\n",
    "\n",
    "```python\n",
    "problem1_samples = problem1_rejection(100000)\n",
    "# ... plotting code ...\n",
    "```\n",
    "- You sample 100,000 points and plot the histogram vs. the true density—**perfect**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 3: Monte Carlo Integral**\n",
    "\n",
    "```python\n",
    "problem1_integral = np.mean(np.sin(problem1_samples))\n",
    "```\n",
    "- **Correct**: You are using the samples to estimate the expectation of `sin(x)` under the given density.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 4: Hoeffding Confidence Interval**\n",
    "\n",
    "```python\n",
    "n = len(problem1_samples)\n",
    "a = 0\n",
    "b = np.sin(1)\n",
    "mean = problem1_integral\n",
    "delta = np.sqrt(np.log(2/0.05) * (b - a) ** 2 / (2 * n))\n",
    "problem1_interval = [mean - delta, mean + delta]\n",
    "```\n",
    "- **Correct use of Hoeffding's inequality** for a 95% confidence interval.\n",
    "- You set `a` and `b` to the min/max values of `sin(x)` in `[0, 1]` (`sin(0)=0`, `sin(1)` is max).\n",
    "- The math and code are correct.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 5: `problem1_rejection_2`**\n",
    "\n",
    "```python\n",
    "def problem1_rejection_2(n_samples=1):\n",
    "    samples = []\n",
    "    M = 21\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1/20)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = 20 * np.exp(20 - 1/x) * (1 + 1/x)\n",
    "        gx = 20  # Uniform on [0, 1/20]\n",
    "        if u < fx / (M * gx):\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "```\n",
    "- **Efficient**: Samples from a uniform proposal, and the scaling factor `M` is reasonable.\n",
    "- The proposal and acceptance ratio are correct for this target.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Remarks**\n",
    "\n",
    "- **All your implementations are correct and efficient** as per the assignment.\n",
    "- Your comments and variable names are clear.\n",
    "- If you want, you may add a random seed before sampling for reproducibility (e.g., `np.random.seed(0)`).\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "Your code fulfills all requirements and is correct for each sub-question. You can submit it as is!\n",
    "\n",
    "If you want the complete code in a single block for copy-paste, just let me know."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
