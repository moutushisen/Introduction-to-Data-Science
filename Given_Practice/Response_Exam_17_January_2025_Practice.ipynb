{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d2dcf8",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Exam 17th of January 2025, 8.00-13.00 for the course 1MS041 (Introduction to Data Science / Introduktion till dataanalys)\n",
    "\n",
    "## Instructions:\n",
    "1. Complete the problems by following instructions.\n",
    "2. When done, submit this file with your solutions saved, following the instruction sheet.\n",
    "\n",
    "This exam has 3 problems for a total of 40 points, to pass you need\n",
    "20 points. The bonus will be added to the score of the exam and rounded afterwards.\n",
    "\n",
    "## Some general hints and information:\n",
    "* Try to answer all questions even if you are uncertain.\n",
    "* Comment your code, so that if you get the wrong answer I can understand how you thought\n",
    "this can give you some points even though the code does not run.\n",
    "* Follow the instruction sheet rigorously.\n",
    "* This exam is partially autograded, but your code and your free text answers are manually graded anonymously.\n",
    "* If there are any questions, please ask the exam guards, they will escalate it to me if necessary.\n",
    "\n",
    "## Tips for free text answers\n",
    "* Be VERY clear with your reasoning, there should be zero ambiguity in what you are referring to.\n",
    "* If you want to include math, you can write LaTeX in the Markdown cells, for instance `$f(x)=x^2$` will be rendered as $f(x)=x^2$ and `$$f(x) = x^2$$` will become an equation line, as follows\n",
    "$$f(x) = x^2$$\n",
    "Another example is `$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$` which renders as\n",
    "$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$\n",
    "\n",
    "## Finally some rules:\n",
    "* You may not communicate with others during the exam, for example:\n",
    "    * You cannot ask for help in Stack-Overflow or other such help forums during the Exam.\n",
    "    * You may not communicate with AI's, for instance ChatGPT.\n",
    "    * Your on-line and off-line activity is being monitored according to the examination rules.\n",
    "\n",
    "## Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a97f3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Insert your anonymous exam ID as a string in the variable below\n",
    "examID=\"0033-TES\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c67547",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea1654",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "This problem is about SVD and anomaly detection. In all the problems where you are asked to produce a matrix or vector, they should be **numpy arrays**. \n",
    "\n",
    "1. [4p] Load the file `data/SVD.csv` as instructed in the code cell. Compute the Singular Value Decomposition, i.e. construct the three matrices $U$, $D$, $V$ such that if $X$ is the data matrix of shape `n_samples x n_dimensions` then $X = UDV^T$. Put the resulting matrices in their variables, check that the shapes align with the instructions in the code cell. Finally, extract the first right and left singular vectors and store those as 1-d arrays in the instructed variables.\n",
    "2. [3p] The first goal is to calculate the explained variance, check the lecture notes for definition. Calculate the explained variance of using $1$, $2$,... number of singular vectors and select how many singular vectors are needed in order to explain at least $95\\%$ of the variance.\n",
    "3. [3p] With the number of components chosen in part 2, construct the best approximating matrix with the rank as the number of components. Explain what each row represents in the approximating matrix in terms of the original data, write your answer as free text in the Markdown cell below as instructed in the cells.\n",
    "4. [4p] Create a vector which corresponds to the row-wise (Euclidean) distance between the original matrix `problem1_data`and the approximating matrix `problem1_approximation` and plot the empirical distribution function of that distance. Based on the empirical distribution function choose a threshold such that 10 samples are above it and the rest below. Store the 10 samples in the instructed variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ea11d5a",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:29.755302Z",
     "iopub.status.busy": "2025-02-03T07:37:29.754951Z",
     "iopub.status.idle": "2025-02-03T07:37:30.543834Z",
     "shell.execute_reply": "2025-02-03T07:37:30.542880Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/SVD.csv' not found. Please make sure the file exists in the 'data' directory.\n",
      "SVD could not be computed because the data file was not found or loaded.\n"
     ]
    }
   ],
   "source": [
    "# Part 1: 4 points\n",
    "import numpy as np\n",
    "# Teacher comment: forgot to import pandas!\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the file data/SVD.csv and store the data in a numpy array called problem1_data below\n",
    "import os\n",
    "\n",
    "file_path = 'data/SVD.csv'\n",
    "if not os.path.exists(file_path):\n",
    "\tprint(f\"File '{file_path}' not found. Please make sure the file exists in the 'data' directory.\")\n",
    "\t# To avoid NameError, assign None or raise an error\n",
    "\tdata = None\n",
    "\tX = None\n",
    "else:\n",
    "\tdata = pd.read_csv(file_path)\n",
    "\tX = data.values\n",
    "\n",
    "if X is not None:\n",
    "\tU, D, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "else:\n",
    "\tU = D = Vt = None\n",
    "\n",
    "if U is not None and Vt is not None:\n",
    "\tfirst_right_singular_vector = Vt[0, :]\n",
    "\tfirst_left_singular_vector = U[:, 0]\n",
    "\n",
    "\tprint(\"Shape of U:\", U.shape)\n",
    "\tprint(\"Shape of D:\", D.shape)\n",
    "\tprint(\"Shape of Vt:\", Vt.shape)\n",
    "else:\n",
    "\tfirst_right_singular_vector = None\n",
    "\tfirst_left_singular_vector = None\n",
    "\tprint(\"SVD could not be computed because the data file was not found or loaded.\")\n",
    "\n",
    "\n",
    "# problem1_U = XXX # The matrix of left singular vectors of problem1_data with shape n_samples x n_dimensions\n",
    "# problem1_D = XXX # The vector of singular values of problem1_data with shape n_dimensions\n",
    "# problem1_V = XXX # The matrix of right singular vectors of problem1_data with shape n_dimensions x n_dimensions\n",
    "\n",
    "# problem1_first_right_singular_vector = XXX # The first right singular vector of problem1_data with shape (n_dimensions,) hint sometimes one needs to invoke flatten() to avoid having shape (n_dimensions, 1) or (1, n_dimensions)\n",
    "# problem1_first_left_singular_vector = XXX # The first left singular vector of problem1_data with shape (n_samples,) hint sometimes one needs to invoke flatten() to avoid having shape (n_samples, 1) or (1, n_samples)\n",
    "\n",
    "if first_right_singular_vector is not None:\n",
    "\tfirst_right_singular_vector = first_right_singular_vector.flatten()\n",
    "if first_left_singular_vector is not None:\n",
    "\tfirst_left_singular_vector = first_left_singular_vector.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "598b5e94",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:30.549788Z",
     "iopub.status.busy": "2025-02-03T07:37:30.549361Z",
     "iopub.status.idle": "2025-02-03T07:37:30.555341Z",
     "shell.execute_reply": "2025-02-03T07:37:30.554375Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [],
   "source": [
    "# Part 2: 3 points\n",
    "\n",
    "# Calculate the explained variance of using 1,2,3,...,n_dimensions singular values and store it as a numpy array called problem1_explained_variance below\n",
    "# problem1_explained_variance = XXX # A numpy array of shape (n_dimensions,), it should be an increasing sequence of positive numbers and the last element should be 1\n",
    "if D is not None:\n",
    "\texplained_variance = (D ** 2) / np.sum(D ** 2)\n",
    "\tcumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\tproblem1_explained_variance = cumulative_explained_variance\n",
    "\t# Store in the variable below the smallest number of singular values needed to explain at least 95% of the variance\n",
    "\t# problem1_num_components = XXX # An integer\n",
    "\tnum_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\tproblem1_num_components = num_components\n",
    "else:\n",
    "\tproblem1_explained_variance = None\n",
    "\tproblem1_num_components = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "517b319e",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:30.560763Z",
     "iopub.status.busy": "2025-02-03T07:37:30.560283Z",
     "iopub.status.idle": "2025-02-03T07:37:30.567973Z",
     "shell.execute_reply": "2025-02-03T07:37:30.566953Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD or number of components not computed. Please check that the data file exists and previous steps ran successfully.\n"
     ]
    }
   ],
   "source": [
    "# Part 3: 3 points\n",
    "\n",
    "# Calculate the approximating matrix of problem1_data using the first problem1_num_components singular values and store it in the variable below\n",
    "# problem1_approximation = XXX # A numpy array of shape n_samples x n_dimensions\n",
    "\n",
    "# Check if SVD has been computed\n",
    "if U is not None and D is not None and Vt is not None and problem1_num_components is not None:\n",
    "\tU_approx = U[:, :problem1_num_components]\n",
    "\tD_approx = np.diag(D[:problem1_num_components])\n",
    "\tVt_approx = Vt[:problem1_num_components, :]\n",
    "\tproblem1_approximation = np.dot(U_approx, np.dot(D_approx, Vt_approx))\n",
    "else:\n",
    "\tproblem1_approximation = None\n",
    "\tprint(\"SVD or number of components not computed. Please check that the data file exists and previous steps ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d90bbc",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put the explanation for **part 3** of the rows of the approximating matrix below this line in this cell. In order to enter edit mode you can doubleclick this cell or select it and just press enter.\n",
    "\n",
    "Explanation : Part 3 is part of a SVD approximation process where U_approx extracts the first num_components number of columns from the matrix U, corresponding to the most significant singular vectors. In D_approx a diagonal matrix is created using the first num_components singular values from D. The diagonal matrix D_approx has only the most significant singular values. Vt_approx just takes the first num_components rows of the transposed matrix Vt, since these correspond to the most significant singular vectors. problem1_approximation Just reconstructs an approximating matrix using the reduced matrices U_approx, D_approx and Vt_approx. Dot product of the matrices would now give the approximation of the original matrix with a reduced rank that retains only the most significant components. In other words, it reduces the dimensionality of the original matrix while retaining most of the significant information. The reconstructed matrix, problem1_approximation, is an approximation to the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ae2296",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:30.573509Z",
     "iopub.status.busy": "2025-02-03T07:37:30.573081Z",
     "iopub.status.idle": "2025-02-03T07:37:33.589889Z",
     "shell.execute_reply": "2025-02-03T07:37:33.587647Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X or problem1_approximation is None. Please ensure previous steps ran successfully.\n"
     ]
    }
   ],
   "source": [
    "# Part 4: 4 points\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if X and problem1_approximation are not None before proceeding\n",
    "if X is not None and problem1_approximation is not None:\n",
    "\t# Calculate the reconstruction error (row-wise Euclidean distance)\n",
    "\tproblem1_reconstruction_error = np.linalg.norm(X - problem1_approximation, axis=1)\n",
    "\trow_wise_distances = problem1_reconstruction_error\n",
    "\n",
    "\t# Plot the empirical distribution function of the reconstruction error\n",
    "\tsns.ecdfplot(row_wise_distances)\n",
    "\tplt.xlabel('Row-wise Euclidean Distance')\n",
    "\tplt.ylabel('ECDF')\n",
    "\tplt.title('Empirical CDF of Reconstruction Error')\n",
    "\tplt.show()\n",
    "\n",
    "\t# Store the value of the selected threshold in the variable below\n",
    "\t# The threshold is the 10th largest value (so that 10 samples are above it)\n",
    "\tproblem1_threshold = np.sort(row_wise_distances)[-10]\n",
    "\n",
    "\t# Store the samples of problem1_data that have a reconstruction error larger than problem1_threshold\n",
    "\t# Should have shape (10, n_dimensions)\n",
    "\tproblem1_outliers = X[row_wise_distances >= problem1_threshold]\n",
    "\tprint(\"Indices of the 10 samples above the threshold:\", np.where(row_wise_distances >= problem1_threshold)[0])\n",
    "else:\n",
    "\tprint(\"X or problem1_approximation is None. Please ensure previous steps ran successfully.\")\n",
    "\tproblem1_reconstruction_error = None\n",
    "\trow_wise_distances = None\n",
    "\tproblem1_threshold = None\n",
    "\tproblem1_outliers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34dda9c3",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:33.598234Z",
     "iopub.status.busy": "2025-02-03T07:37:33.597520Z",
     "iopub.status.idle": "2025-02-03T07:37:33.900466Z",
     "shell.execute_reply": "2025-02-03T07:37:33.899448Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "TEST",
    "lx_problem_number": "1",
    "lx_problem_points": "14",
    "lx_test_only": "True"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tests for problem 1\n",
      "\n",
      "---------------------------------\n",
      "Beginning test for part1\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_data' is not defined\n",
      "The shape of the data is not correct\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_data' is not defined\n",
      "The SVD decomposition does not recover the original matrix\n",
      "You got 2.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_U' is not defined\n",
      "The left singular vectors have the wrong shape\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_D' is not defined\n",
      "The singular values have the wrong shape\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_V' is not defined\n",
      "The right singular vectors have the wrong shape\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_first_right_singular_vector' is not defined\n",
      "The first right singular vector is incorrect\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_first_left_singular_vector' is not defined\n",
      "The first left singular vector is incorrect\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_data' is not defined\n",
      "The right singular vectors do not recover the singular values\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_V' is not defined\n",
      "The right singular vectors are not orthonormal\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_U' is not defined\n",
      "The left singular vectors are not orthonormal\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part2\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_explained_variance' is not defined\n",
      "The explained variance has the wrong shape\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_explained_variance' is not defined\n",
      "The explained variance is not increasing\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_explained_variance' is not defined\n",
      "The explained variance does not end in 1\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_explained_variance' is not defined\n",
      "Each entry in the explained variance that is wrong gives deduced points\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_num_components' is not defined\n",
      "The number of components is incorrect\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part3\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_data' is not defined\n",
      "The approximation is incorrect using your supplied matrices and your calculated number of components\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part4\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_reconstruction_error' is not defined\n",
      "The reconstruction error is incorrect\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_reconstruction_error' is not defined\n",
      "The number of outliers is incorrect\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem1_outliers' is not defined\n",
      "The outliers are incorrect\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment\n",
      "\n",
      "All tests complete, you got = 3.00 points\n",
      "The number of points you have scored for this problem is 3.0 out of 14\n",
      "The number of points you have accumulated thus far is   3.0 out of 14\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4d0d75",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3b664ad",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are just numbers in the dataset $0,1,2,\\ldots$ and each row consists of a user, a source and a destination page. This signifies that the user was on the source page and clicked a link leading them to the destination page. The goal is to improve the user experience by decreasing load time of the next page visited, as such we need a good estimate for the next site likely to be visited. We will model this using a homogeneous Markov chain, each row in the data-file then corresponds to a single realization of a transition. \n",
    "\n",
    "1. [3p] Load the data in the file `data/websites.csv` and construct a matrix of size `n_pages x n_pages` which is the maximum likelihood estimate of the true transition matrix for the Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is page $0$ has index $0$ in the matrix.\n",
    "2. [4p] A page loads in $\\text{Exp}(1)$ (Exponentially distributed with mean $1$) seconds if not preloaded and loads with $\\text{Exp}(10)$ (Exponentially distributed with mean $1/10$) seconds if preloaded and we only preload the most likely next site. Given that we start in page $1$ simulate $10000$ load times from page $1$ (that is, only a single step), store the result in the variable indicated in the cell.\n",
    "Repeat the experiment but this time preload the two most likely pages and store the result in the indicated variable.\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in the free text field).\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load time with respect to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "521439a9",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:33.908062Z",
     "iopub.status.busy": "2025-02-03T07:37:33.906986Z",
     "iopub.status.idle": "2025-02-03T07:37:34.023779Z",
     "shell.execute_reply": "2025-02-03T07:37:34.022441Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/websites.csv' not found. Please make sure the file exists in the 'data' directory.\n"
     ]
    }
   ],
   "source": [
    "# Part 1: 3 points\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the data from the file data/websites.csv and estimate the transition matrix of the Markov chain\n",
    "websites_file_path = 'data/websites.csv'\n",
    "if not os.path.exists(websites_file_path):\n",
    "    print(f\"File '{websites_file_path}' not found. Please make sure the file exists in the 'data' directory.\")\n",
    "    data = None\n",
    "    transition_matrix = None\n",
    "    n_pages = None\n",
    "else:\n",
    "    data = pd.read_csv(websites_file_path)\n",
    "    n_pages = data['source'].max() + 1\n",
    "    transition_matrix = np.zeros((n_pages, n_pages))\n",
    "    for _, row in data.iterrows():\n",
    "        transition_matrix[row['source'], row['destination']] += 1\n",
    "    transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Store the estimated transition matrix in the variable problem2_transition_matrix below\n",
    "# problem2_transition_matrix = XXX # A numpy array of shape (problem2_n_states, problem2_n_states)\n",
    "\n",
    "# Store the number of states in the variable problem2_n_states below\n",
    "# problem2_n_states = XXX # An integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3adcdd2",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:34.030435Z",
     "iopub.status.busy": "2025-02-03T07:37:34.029812Z",
     "iopub.status.idle": "2025-02-03T07:37:35.156944Z",
     "shell.execute_reply": "2025-02-03T07:37:35.155895Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_matrix or n_pages is None. Please ensure the data file exists and previous steps ran successfully.\n"
     ]
    }
   ],
   "source": [
    "# Part 2: 4 points\n",
    "np.random.seed(42)\n",
    "n_simulations = 10000\n",
    "\n",
    "def simulate_load_times(transition_matrix, n_pages, start_page, preload_pages=1):\n",
    "    load_times = []\n",
    "    for _ in range(n_simulations):\n",
    "        next_page = np.random.choice(n_pages, p=transition_matrix[start_page])\n",
    "        if next_page in np.argsort(transition_matrix[start_page])[-preload_pages:]:\n",
    "            load_times.append(np.random.exponential(1/10))\n",
    "        else:\n",
    "            load_times.append(np.random.exponential(1))\n",
    "    return np.array(load_times)\n",
    "\n",
    "# Check if transition_matrix and n_pages are available before running the simulation\n",
    "if transition_matrix is not None and n_pages is not None:\n",
    "    # Simulate the website load times for the next page of 10000 users that are currently on page 1 (recall indexing starts at 0) when we only load the most likely page.\n",
    "    # Store the simulated page load times in the variable problem2_page_load_times_top below\n",
    "    # problem2_page_load_times_top = XXX # A numpy array of shape (10000,)\n",
    "    load_times_1 = simulate_load_times(transition_matrix, n_pages, 1, preload_pages=1)\n",
    "\n",
    "    # Repeat the simulation of load times for the next page of 10000 users that are currently on page 1 when we load the two most likely pages.\n",
    "    # Store the simulated page load times in the variable problem2_page_load_times_two below\n",
    "    # problem2_page_load_times_two = XXX # A numpy array of shape (10000,)\n",
    "    load_times_2 = simulate_load_times(transition_matrix, n_pages, 1, preload_pages=2)\n",
    "else:\n",
    "    print(\"transition_matrix or n_pages is None. Please ensure the data file exists and previous steps ran successfully.\")\n",
    "    load_times_1 = None\n",
    "    load_times_2 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b5eccd1",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.161344Z",
     "iopub.status.busy": "2025-02-03T07:37:35.160991Z",
     "iopub.status.idle": "2025-02-03T07:37:35.168237Z",
     "shell.execute_reply": "2025-02-03T07:37:35.167508Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average load time without preloading: 1.0185314634657625\n",
      "Average load time with preloading 1 page: None\n",
      "Average load time with preloading 2 pages: None\n"
     ]
    }
   ],
   "source": [
    "# Part 3: 3 points\n",
    "\n",
    "# Calculate the true expected load time for loading a page without pre-loading the next page and store it in the variable below\n",
    "# problem2_avg = XXX # A float\n",
    "average_load_time_no_preload = np.mean(np.random.exponential(1, n_simulations))\n",
    "\n",
    "if load_times_1 is not None:\n",
    "\taverage_load_time_1 = np.mean(load_times_1)\n",
    "else:\n",
    "\taverage_load_time_1 = None\n",
    "\n",
    "if load_times_2 is not None:\n",
    "\taverage_load_time_2 = np.mean(load_times_2)\n",
    "else:\n",
    "\taverage_load_time_2 = None\n",
    "\n",
    "# Is the average load time for loading a page without pre-loading the next page larger than the average load time for loading a page after pre-loading the next most likely page?\n",
    "# problem2_comparison = XXX # True / False\n",
    "if (average_load_time_1 is not None) and (average_load_time_2 is not None):\n",
    "\tproblem2_comparison = average_load_time_no_preload > average_load_time_1\n",
    "else:\n",
    "\tproblem2_comparison = None\n",
    "\n",
    "print(f\"Average load time without preloading: {average_load_time_no_preload}\")\n",
    "print(f\"Average load time with preloading 1 page: {average_load_time_1}\")\n",
    "print(f\"Average load time with preloading 2 pages: {average_load_time_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f007420",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put the explanation for **part 3** of how you made the decision about `problem2_comparison` below this line in this **cell**. In order to enter edit mode you can doubleclick this cell or select it and press enter.\n",
    "\n",
    "Explanation: It compares the average load time for loading a page without pre-loading the next page larger than the average load time for loading a page after pre-loading the next most likely page. The average value of a function is found by taking the integral of the function over the interval and dividing by the length of the interval. mean() method calculates the mean (average) of the given data set and then divide by how many values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbfec65d",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.172773Z",
     "iopub.status.busy": "2025-02-03T07:37:35.172427Z",
     "iopub.status.idle": "2025-02-03T07:37:35.181644Z",
     "shell.execute_reply": "2025-02-03T07:37:35.180959Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_matrix or n_pages is None. Please ensure previous steps ran successfully.\n"
     ]
    }
   ],
   "source": [
    "# Part 4: 4 points\n",
    "\n",
    "# Begin by calculating the stationary distribution of the Markov chain and store it in the variable below\n",
    "# WARNING: Since the transition matrix is not symmetric, numpy might make the output of the eigenvectors complex, you can use np.real() to get the real part of the eigenvectors\n",
    "# Store the stationary distribution in the variable below called problem2_stationary_distribution\n",
    "# problem2_stationary_distribution = XXX # A numpy array of shape (problem2_n_states,)\n",
    "\n",
    "if transition_matrix is not None and n_pages is not None:\n",
    "    stationary_distribution = np.linalg.matrix_power(transition_matrix, 1000)[0]\n",
    "    problem2_stationary_distribution = stationary_distribution\n",
    "\n",
    "    # Now use the above stationary distribution to calculate the average load time for loading a page after pre-loading the next most likely page according to the stationary distribution\n",
    "    # Store the average load time in the variable below\n",
    "    # problem2_avg_stationary = XXX # A float\n",
    "    expected_load_time = 0\n",
    "    for i in range(n_pages):\n",
    "        for j in range(n_pages):\n",
    "            if j in np.argsort(transition_matrix[i])[-2:]:\n",
    "                expected_load_time += stationary_distribution[i] * transition_matrix[i, j] * (1/10)\n",
    "            else:\n",
    "                expected_load_time += stationary_distribution[i] * transition_matrix[i, j] * 1\n",
    "\n",
    "    problem2_avg_stationary = expected_load_time\n",
    "    print(f\"Expected load time with respect to the stationary distribution: {expected_load_time}\")\n",
    "else:\n",
    "    print(\"transition_matrix or n_pages is None. Please ensure previous steps ran successfully.\")\n",
    "    problem2_stationary_distribution = None\n",
    "    problem2_avg_stationary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "587b6d7f",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.186766Z",
     "iopub.status.busy": "2025-02-03T07:37:35.186381Z",
     "iopub.status.idle": "2025-02-03T07:37:35.349721Z",
     "shell.execute_reply": "2025-02-03T07:37:35.348920Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "TEST",
    "lx_problem_number": "2",
    "lx_problem_points": "14",
    "lx_test_only": "True"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tests for problem 2\n",
      "\n",
      "---------------------------------\n",
      "Beginning test for part1\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_transition_matrix' is not defined\n",
      "Your transition matrix is not of the correct shape, should be (10,10)\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_transition_matrix' is not defined\n",
      "You get a deduction for each mistake in the transition matrix\n",
      "You got 1.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_transition_matrix' is not defined\n",
      "Your matrix is not a transition matrix, the rows do not sum to 1\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_n_states' is not defined\n",
      "The number of states is not correct, should be 10\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part2\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_transition_matrix' is not defined\n",
      "Your problem2_page_load_times_top is too far away from the true distribution\n",
      "You got 2.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_transition_matrix' is not defined\n",
      "Your problem2_page_load_times_two is too far away from the true distribution\n",
      "You got 2.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part3\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_avg' is not defined\n",
      "Your average load time is not correct, should be 1\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_comparison' is not defined\n",
      "Your comparison is not correct, should be True\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part4\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_stationary_distribution' is not defined\n",
      "The stationary distribution does not sum to 1\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_stationary_distribution' is not defined\n",
      "The stationary distribution is not real, you forgot to take the real part of the eigenvector\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_stationary_distribution' is not defined\n",
      "You get a deduction for each mistake in the stationary distribution\n",
      "You got 1.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_avg_stationary' is not defined\n",
      "Your average load time is not correct, should be roughly 0.87...\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem2_avg_stationary' is not defined\n",
      "Your average load time is not correct, should be roughly 0.87... but you are allowed a small error if you for instance simulated the load times\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "\n",
      "All tests complete, you got = 2.00 points\n",
      "The number of points you have scored for this problem is 2.0 out of 14\n",
      "The number of points you have accumulated thus far is   5.0 out of 28\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08beccb8",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21deee81",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "source": [
    "\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "* **True Positive (TP)**: Detecting fraud and blocking the transaction costs the company 100 (manual review etc.)\n",
    "* **True Negative (TN)**: Allowing a legitimate transaction has no cost.\n",
    "* **False Positive (FP)**: Incorrectly classifying a legitimate transaction as fraudulent costs 120 (customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "* **False Negative (FN)**: Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "**The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES**\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on the validation data. Also calculate the precision and recall at the optimal threshold on the validation data on class 1 and 0.\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the $0-1$ loss. Calculate the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "3. [4p] Provide a confidence interval around the optimal cost (with $95\\%$ confidence) applied to the test data and explain all the assumption you made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f1aadd3",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.354540Z",
     "iopub.status.busy": "2025-02-03T07:37:35.354180Z",
     "iopub.status.idle": "2025-02-03T07:37:35.620099Z",
     "shell.execute_reply": "2025-02-03T07:37:35.619113Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/fraud.csv' not found. Please make sure the file exists in the 'data' directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RUN THIS CELL TO GET THE DATA\n",
    "\n",
    "# We start by loading the data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "fraud_file_path = 'data/fraud.csv'\n",
    "if not os.path.exists(fraud_file_path):\n",
    "\tprint(f\"File '{fraud_file_path}' not found. Please make sure the file exists in the 'data' directory.\")\n",
    "else:\n",
    "\tPROBLEM3_DF = pd.read_csv(fraud_file_path)\n",
    "\tY = PROBLEM3_DF['Class'].values\n",
    "\tX = PROBLEM3_DF[['V%d' % i for i in range(1,5)]+['Amount']].values\n",
    "\n",
    "\t# We will split the data into training, testing and validation sets\n",
    "\tfrom Utils import train_test_validation\n",
    "\tPROBLEM3_X_train, PROBLEM3_X_test, PROBLEM3_X_val, PROBLEM3_y_train, PROBLEM3_y_test, PROBLEM3_y_val = train_test_validation(X,Y,shuffle=True,random_state=1)\n",
    "\n",
    "\t# From this we will train a logistic regression model\n",
    "\tfrom sklearn.linear_model import LogisticRegression\n",
    "\tlr = LogisticRegression()\n",
    "\tlr.fit(PROBLEM3_X_train,PROBLEM3_y_train)\n",
    "\n",
    "\t# THE FOLLOWING CODE WILL PRODUCE THE ARRAYS YOU NEED FOR THE PROBLEM\n",
    "\n",
    "\tPROBLEM3_y_pred_proba_val = lr.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "\tPROBLEM3_y_true_val = PROBLEM3_y_val\n",
    "\n",
    "\tPROBLEM3_y_pred_proba_test = lr.predict_proba(PROBLEM3_X_test)[:,1]\n",
    "\tPROBLEM3_y_true_test = PROBLEM3_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c588e528",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.625748Z",
     "iopub.status.busy": "2025-02-03T07:37:35.625302Z",
     "iopub.status.idle": "2025-02-03T07:37:35.634593Z",
     "shell.execute_reply": "2025-02-03T07:37:35.633247Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Part 1: 3 points\n",
    "# Implement the following function that calculates the cost of a binary classifier according to\n",
    "# the specification in the problem statement\n",
    "# See the comments inside the function for details of the parameters\n",
    "\n",
    "#def cost(y_true,y_predict_proba,threshold):\n",
    "    # y_true is a numpy array of shape (n_samples,) with binary labels\n",
    "    # y_predict_proba is a numpy array of shape (n_samples,) with predicted probabilities\n",
    "    # threshold is a float between 0 and 1\n",
    "\n",
    "    # When returning the cost, you should return the average cost per sample\n",
    "    # thus it should be a value\n",
    "\n",
    "def cost(y_true, y_pred, threshold):\n",
    "    TP = np.sum((y_pred >= threshold) & (y_true == 1))\n",
    "    TN = np.sum((y_pred < threshold) & (y_true == 0))\n",
    "    FP = np.sum((y_pred >= threshold) & (y_true == 0))\n",
    "    FN = np.sum((y_pred < threshold) & (y_true == 1))\n",
    "    \n",
    "    total_cost = (TP * 100) + (FP * 120) + (FN * 600)\n",
    "    return total_cost / len(y_true)\n",
    "\n",
    "    # return XXX # A float\n",
    "\n",
    "# Provide the code below to plot the cost as a function of the threshold\n",
    "# using the validation data, specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "# The plot should be between 0 and 1 with 0.01 increments\n",
    "# The y-axis should be the cost and the x-axis should be the threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25387d22",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.639679Z",
     "iopub.status.busy": "2025-02-03T07:37:35.639216Z",
     "iopub.status.idle": "2025-02-03T07:37:35.664874Z",
     "shell.execute_reply": "2025-02-03T07:37:35.663684Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM3_y_true_val and/or PROBLEM3_y_pred_proba_val are not defined. Please ensure the data file exists and the previous cell has been executed.\n"
     ]
    }
   ],
   "source": [
    "# Part 2: 2.5 points\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if the required variables are defined before proceeding\n",
    "if 'PROBLEM3_y_true_val' in globals() and 'PROBLEM3_y_pred_proba_val' in globals():\n",
    "\t# Use the cost function you just implemented above to find the threshold that minimizes the cost\n",
    "\t# using the validation data, specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "\t# Store the threshold in the variable below\n",
    "\n",
    "\t# problem3_threshold = XXX # A float between 0 and 1\n",
    "\tthresholds = np.arange(0, 1.01, 0.01)\n",
    "\tcosts = np.zeros_like(thresholds)\n",
    "\n",
    "\t# Now calculate the cost of the classifier using the validation data and the threshold you just found\n",
    "\t# using the validation data, specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "\t# Store the cost in the variable below\n",
    "\n",
    "\t# problem3_cost_val = XXX # A float\n",
    "\tfor i, threshold in enumerate(thresholds):\n",
    "\t\tcosts[i] = cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, threshold)\n",
    "\n",
    "\t# Using the threshold you just found, calculate the predicted labels of the classifier on the validation data\n",
    "\t# put the predicted labels in the variable below\n",
    "\t# problem3_y_pred_val = XXX # A numpy array of shape (n_samples,) with values 0 or 1\n",
    "\n",
    "\toptimal_threshold_cost = thresholds[np.argmin(costs)]\n",
    "\tmin_cost = costs[np.argmin(costs)]\n",
    "\n",
    "\t# Calculate the precision and recall of the classifier of class 1 using the threshold you just found\n",
    "\t# using the validation data, specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "\n",
    "\t# problem3_precision_1 = XXX # A float between 0 and 1\n",
    "\t# problem3_recall_1 = XXX # A float between 0 and 1\n",
    "\n",
    "\ty_pred_optimal_cost = (PROBLEM3_y_pred_proba_val >= optimal_threshold_cost).astype(int)\n",
    "\n",
    "\t# Calculate the precision and recall of the classifier of class 0 using the threshold you just found\n",
    "\t# using the validation data, specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "\n",
    "\t# problem3_precision_0 = XXX # A float between 0 and 1\n",
    "\t# problem3_recall_0 = XXX # A float between 0 and 1\n",
    "\n",
    "\tprecision_optimal_cost = precision_score(PROBLEM3_y_true_val, y_pred_optimal_cost)\n",
    "\trecall_optimal_cost = recall_score(PROBLEM3_y_true_val, y_pred_optimal_cost)\n",
    "else:\n",
    "\tprint(\"PROBLEM3_y_true_val and/or PROBLEM3_y_pred_proba_val are not defined. Please ensure the data file exists and the previous cell has been executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0362a128",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.670548Z",
     "iopub.status.busy": "2025-02-03T07:37:35.669801Z",
     "iopub.status.idle": "2025-02-03T07:37:35.676454Z",
     "shell.execute_reply": "2025-02-03T07:37:35.675397Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [],
   "source": [
    "# Part 3: 2.5 points\n",
    "\n",
    "# Find the threshold that minimizes the $0-1$ loss using the validation data\n",
    "# specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "# Store the threshold in the variable below\n",
    "\n",
    "# problem3_threshold_01 = XXX # A float between 0 and 1\n",
    "def zero_one_loss(y_true, y_pred, threshold):\n",
    "    return np.mean((y_pred >= threshold) != y_true)\n",
    "\n",
    "# Now calculate the difference in cost (using the cost function you implemented in step 1) between the optimal one chosen in part 2 and the one chosen in part 3 by taking the cost with the threshold found in part 3 and subtracting the cost with the threshold found in part 2 to get a positive value\n",
    "# problem3_cost_difference = XXX # A float\n",
    "losses = np.zeros_like(thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb3d73e4",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.682518Z",
     "iopub.status.busy": "2025-02-03T07:37:35.681618Z",
     "iopub.status.idle": "2025-02-03T07:37:35.751208Z",
     "shell.execute_reply": "2025-02-03T07:37:35.749999Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM3_y_true_test and/or PROBLEM3_y_pred_proba_test are not defined. Please run the cell that loads and prepares the fraud data.\n"
     ]
    }
   ],
   "source": [
    "# Part 4: 4 points\n",
    "\n",
    "# Using the threshold problem3_threshold use Hoeffdings inequality to provide a confidence interval \n",
    "# for the cost of the classifier with 95 % confidence using the test data.\n",
    "# Specifically the arrays PROBLEM3_y_true_test and PROBLEM3_y_pred_proba_test.\n",
    "# Store the lower and upper bounds of the confidence interval in the variables below\n",
    "\n",
    "# problem3_lower_bound = XXX # A float\n",
    "# problem3_upper_bound = XXX # A float\n",
    "\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "# Check if test arrays are defined\n",
    "if 'PROBLEM3_y_true_test' in globals() and 'PROBLEM3_y_pred_proba_test' in globals():\n",
    "\toptimal_threshold_cost = thresholds[np.argmin(costs)]\n",
    "\t# Calculate the cost on the test set at the optimal threshold for cost\n",
    "\ttest_costs = []\n",
    "\tfor i in range(len(PROBLEM3_y_true_test)):\n",
    "\t\ttest_costs.append(cost(np.array([PROBLEM3_y_true_test[i]]), np.array([PROBLEM3_y_pred_proba_test[i]]), optimal_threshold_cost))\n",
    "\ttest_costs = np.array(test_costs)\n",
    "\n",
    "\tconfidence = 0.95\n",
    "\tn = len(test_costs)\n",
    "\tmean_cost = np.mean(test_costs)\n",
    "\th = sem(test_costs) * t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "\tproblem3_lower_bound = mean_cost - h\n",
    "\tproblem3_upper_bound = mean_cost + h\n",
    "\n",
    "\tprint(f\"95% confidence interval around the optimal cost: [{problem3_lower_bound:.2f}, {problem3_upper_bound:.2f}]\")\n",
    "else:\n",
    "\tprint(\"PROBLEM3_y_true_test and/or PROBLEM3_y_pred_proba_test are not defined. Please run the cell that loads and prepares the fraud data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67721024",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put your explanation for part 4 below this line in this **cell**. Doubleclick to enter edit mode as before.\n",
    "\n",
    "Explanation: Hoeffding's inequality to give a confidence interval for the cost of the classifier with 95 % confidence using the test data.\n",
    "These are namely the arrays PROBLEM3_y_true_test and PROBLEM3_y_pred_proba_test which store the lower and upper bounds of the confidence interval in the variables below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918ea7a3",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2025-02-03T07:37:35.757015Z",
     "iopub.status.busy": "2025-02-03T07:37:35.756551Z",
     "iopub.status.idle": "2025-02-03T07:37:35.817742Z",
     "shell.execute_reply": "2025-02-03T07:37:35.816981Z"
    },
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "TEST",
    "lx_problem_number": "3",
    "lx_problem_points": "12",
    "lx_test_only": "True"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tests for problem 3\n",
      "\n",
      "---------------------------------\n",
      "Beginning test for part1\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "Your cost function is correct on a small example using threshold 0.5\n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "Your cost function is correct on a small example using threshold 0.2\n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "Your cost function is correct on the validation data using threshold 0.1\n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: -1\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part2\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_cost_val' is not defined\n",
      "Your threshold does not minimize the cost you supplied\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_threshold' is not defined\n",
      "Your cost is not correct using the threshold you found\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_y_pred_val' is not defined\n",
      "You get a deduction for each mistake in the predicted labels\n",
      "You got 0.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_precision_1' is not defined\n",
      "Your precision for class 1 is not correct\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_recall_1' is not defined\n",
      "Your recall for class 1 is not correct\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_precision_0' is not defined\n",
      "Your precision for class 0 is not correct\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_recall_0' is not defined\n",
      "Your recall for class 0 is not correct\n",
      "You got 0.1 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part3\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_threshold_01' is not defined\n",
      "Your zero-one loss at the supplied threshold does not minimize the zero-one loss\n",
      "You got 1.5 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_cost_difference' is not defined\n",
      "Your cost difference is not correct\n",
      "You got 1.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "---------------------------------\n",
      "Beginning test for part4\n",
      "---------------------------------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_threshold' is not defined\n",
      "Your lower bound is not correct\n",
      "You got 2.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "-----Beginning test------\n",
      "name 'problem3_threshold' is not defined\n",
      "Your upper bound is not correct\n",
      "You got 2.0 points deduction \n",
      "-----Ending test---------\n",
      "\n",
      "Manual points: 0\n",
      "No comment!\n",
      "\n",
      "All tests complete, you got = 2.00 points\n",
      "The number of points you have scored for this problem is 2.0 out of 12\n",
      " \n",
      " \n",
      " \n",
      "The number of points you have scored in total for this entire set of Problems is 7.0 out of 40\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lx_assignment_number": "vB",
  "lx_course_instance": "2024",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
