{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Part 1: Load and preprocess the data\n",
    "data = pd.read_csv(\"data/Corona_NLP_train.csv\", encoding='latin1')\n",
    "data = data[data['Sentiment'] != 'Neutral']  # Remove Neutral tweets\n",
    "\n",
    "# Map sentiment to binary values\n",
    "data['Sentiment'] = data['Sentiment'].map({'Positive': 1, 'Extremely Positive': 1, 'Negative': 0, 'Extremely Negative': 0})\n",
    "\n",
    "# Extract X and Y\n",
    "X = data['OriginalTweet'].values\n",
    "Y = data['Sentiment'].values\n",
    "\n",
    "# Split data (train 60%, test 15%, validation 25%)\n",
    "train_size = int(0.6 * len(X))\n",
    "test_size = int(0.15 * len(X))\n",
    "valid_size = len(X) - train_size - test_size\n",
    "\n",
    "X_train, X_test, X_valid = X[:train_size], X[train_size:train_size+test_size], X[train_size+test_size:]\n",
    "Y_train, Y_test, Y_valid = Y[:train_size], Y[train_size:train_size+test_size], Y[train_size+test_size:]\n",
    "\n",
    "# Part 2: Create and train the pipeline model\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Part 3: Evaluate precision and recall\n",
    "Y_pred_test = model.predict(X_test)\n",
    "precision_0 = precision_score(Y_test, Y_pred_test, pos_label=0)\n",
    "precision_1 = precision_score(Y_test, Y_pred_test, pos_label=1)\n",
    "recall_0 = recall_score(Y_test, Y_pred_test, pos_label=0)\n",
    "recall_1 = recall_score(Y_test, Y_pred_test, pos_label=1)\n",
    "\n",
    "# Part 4: Define cost function\n",
    "def cost(model, threshold, X, Y):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    predictions = (probs >= threshold).astype(int)\n",
    "    cost_values = (1 - predictions) * Y + 5 * (1 - Y) * predictions\n",
    "    return np.mean(cost_values)\n",
    "\n",
    "# Part 5: Find optimal threshold\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "costs = [cost(model, t, X_test, Y_test) for t in thresholds]\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "cost_at_optimal_threshold = min(costs)\n",
    "\n",
    "# Part 6: Evaluate cost and confidence interval on validation set\n",
    "cost_at_optimal_threshold_valid = cost(model, optimal_threshold, X_valid, Y_valid)\n",
    "n_valid = len(X_valid)\n",
    "epsilon = np.sqrt(np.log(1 / 0.01) / (2 * n_valid))\n",
    "cost_interval_valid = (\n",
    "    cost_at_optimal_threshold_valid - epsilon,\n",
    "    cost_at_optimal_threshold_valid + epsilon\n",
    ")\n",
    "\n",
    "# Part 7: Compute empirical variance and confidence interval using Bennett's inequality\n",
    "probs_valid = model.predict_proba(X_valid)[:, 1]\n",
    "predictions_valid = (probs_valid >= optimal_threshold).astype(int)\n",
    "cost_values = (1 - predictions_valid) * Y_valid + 5 * (1 - Y_valid) * predictions_valid\n",
    "variance_of_C = np.var(cost_values)\n",
    "delta = np.sqrt((2 * variance_of_C * np.log(1 / 0.01)) / n_valid) + (7 * np.log(1 / 0.01) / (3 * n_valid))\n",
    "interval_of_C = (\n",
    "    cost_at_optimal_threshold_valid - delta,\n",
    "    cost_at_optimal_threshold_valid + delta\n",
    ")\n",
    "\n",
    "# Results\n",
    "results = {\n",
    "    \"precision_0\": precision_0,\n",
    "    \"precision_1\": precision_1,\n",
    "    \"recall_0\": recall_0,\n",
    "    \"recall_1\": recall_1,\n",
    "    \"optimal_threshold\": optimal_threshold,\n",
    "    \"cost_at_optimal_threshold\": cost_at_optimal_threshold,\n",
    "    \"cost_at_optimal_threshold_valid\": cost_at_optimal_threshold_valid,\n",
    "    \"cost_interval_valid\": cost_interval_valid,\n",
    "    \"variance_of_C\": variance_of_C,\n",
    "    \"interval_of_C\": interval_of_C,\n",
    "}\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Data Loading and Preprocessing:\n",
    "\n",
    "Removed rows with Neutral sentiment.\n",
    "Mapped positive sentiments to 1 and negative sentiments to 0.\n",
    "Split the data into train, test, and validation sets in the required proportions.\n",
    "Pipeline Creation:\n",
    "\n",
    "Combined CountVectorizer and LogisticRegression into a single Pipeline for streamlined training and inference.\n",
    "Precision and Recall Calculation:\n",
    "\n",
    "Used precision_score and recall_score from sklearn to evaluate model performance on the test set.\n",
    "Cost Function Implementation:\n",
    "\n",
    "Computed the average cost for a given threshold using the predicted probabilities.\n",
    "Optimal Threshold Selection:\n",
    "\n",
    "Evaluated the cost over a range of thresholds and selected the one minimizing the cost.\n",
    "Validation Cost and Confidence Interval:\n",
    "\n",
    "Applied Hoeffding's inequality for the confidence interval.\n",
    "Empirical Variance and Bennett's Confidence Interval:\n",
    "\n",
    "Calculated variance and refined the confidence interval using Bennett's inequality.\n",
    "This solution adheres to the problem requirements and includes all steps for evaluation and optimization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
